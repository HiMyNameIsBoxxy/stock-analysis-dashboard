{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c736a0b",
   "metadata": {},
   "source": [
    "# 1. Imports & Constants  \n",
    "Import all necessary packages and set up constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Constants\n",
    "KEYWORDS = [\"tesla\", \"apple\", \"google\", \"nvidia\", \"microsoft\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16b5dd",
   "metadata": {},
   "source": [
    "# 2. Cluster Configuration & Environment  \n",
    "Load environment variables and define MongoDB clusters with date ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB clusters\n",
    "CLUSTERS = [\n",
    "    {\n",
    "        \"uri\": os.getenv(\"MONGO_ATLAS_URI_1\"),\n",
    "        \"start_date\": \"2024-01-01T00:00:00Z\",\n",
    "        \"end_date\": \"2024-03-01T23:59:59Z\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": os.getenv(\"MONGO_ATLAS_URI_2\"),\n",
    "        \"start_date\": \"2024-03-02T00:00:00Z\",\n",
    "        \"end_date\": \"2024-04-26T23:59:59Z\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": os.getenv(\"MONGO_ATLAS_URI_3\"),\n",
    "        \"start_date\": \"2024-04-27T00:00:00Z\",\n",
    "        \"end_date\": \"2024-06-30T23:59:59Z\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d03e4f",
   "metadata": {},
   "source": [
    "# 3. Database Connection Helper  \n",
    "Define a function to connect and verify to a MongoDB collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(uri):\n",
    "    client = MongoClient(\n",
    "        uri,\n",
    "        serverSelectionTimeoutMS=10_000,\n",
    "        connectTimeoutMS=10_000,\n",
    "        socketTimeoutMS=60_000,\n",
    "    )\n",
    "    # verify the connection early\n",
    "    client.admin.command(\"ping\")\n",
    "    return client[\"gdelt_news\"][\"articles\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d93663",
   "metadata": {},
   "source": [
    "# 4. Fetching Company Articles  \n",
    "Query each cluster collection for articles matching our keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca36e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_articles(collection, start_date, end_date):\n",
    "    company_articles = defaultdict(list)\n",
    "    base_query = {\n",
    "        \"date\": {\"$gte\": start_date, \"$lte\": end_date}\n",
    "    }\n",
    "\n",
    "    for company in KEYWORDS:\n",
    "        q = {\n",
    "            **base_query,\n",
    "            \"$or\": [\n",
    "                {\"title\": {\"$regex\": company, \"$options\": \"i\"}},\n",
    "                {\"url\":   {\"$regex\": company, \"$options\": \"i\"}}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        cursor = (collection\n",
    "                  .find(q, {\"_id\":1, \"date\":1, \"title\":1})\n",
    "                  .batch_size(1000))\n",
    "        \n",
    "        count = 0\n",
    "        for doc in cursor:\n",
    "            company_articles[company].append(doc)\n",
    "            count += 1\n",
    "\n",
    "        print(f\"Found {count} articles for {company}\")\n",
    "\n",
    "    return company_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3c9b0",
   "metadata": {},
   "source": [
    "# 5. Sentiment Analysis Function  \n",
    "Analyze sentiment polarity of each article title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(title):\n",
    "    \"\"\"Analyze sentiment of article title\"\"\"\n",
    "    analysis = TextBlob(title)\n",
    "    return analysis.sentiment.polarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6baf832",
   "metadata": {},
   "source": [
    "# 6. Create Daily Aggregates  \n",
    "Build a DataFrame of daily article counts and average sentiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_daily_aggregates(company_articles, start_date, end_date):\n",
    "    \"\"\"Create a DataFrame with daily aggregates for each company\"\"\"\n",
    "    # Convert string dates to datetime\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # Create date range\n",
    "    date_range = pd.date_range(start=start_dt, end=end_dt, freq='D')\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    daily_data = []\n",
    "    \n",
    "    for company in company_articles:\n",
    "        daily_counts = defaultdict(int)\n",
    "        daily_sentiments = defaultdict(list)\n",
    "        \n",
    "        for article in company_articles[company]:\n",
    "            article_date = datetime.strptime(article['date'], \"%Y-%m-%dT%H:%M:%SZ\").date()\n",
    "            daily_counts[article_date] += 1\n",
    "            sentiment = analyze_sentiment(article['title'])\n",
    "            daily_sentiments[article_date].append(sentiment)\n",
    "        \n",
    "        # Calculate aggregates for each day\n",
    "        for date in date_range:\n",
    "            date_obj = date.date()\n",
    "            count = daily_counts[date_obj]\n",
    "            avg_sentiment = np.mean(daily_sentiments[date_obj]) if daily_sentiments[date_obj] else 0\n",
    "            \n",
    "            daily_data.append({\n",
    "                'date': date_obj,\n",
    "                'company': company,\n",
    "                'article_count': count,\n",
    "                'avg_sentiment': avg_sentiment\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(daily_data)\n",
    "    \n",
    "    # Sort by date and company\n",
    "    df = df.sort_values(['date', 'company'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fc078",
   "metadata": {},
   "source": [
    "# 7. Merge with Stock Prices  \n",
    "Combine the article aggregates with daily OHLCV, forward‑ and back‑filling missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_stock_prices(articles_df, stock_prices_path):\n",
    "    \"\"\"Merge article data with stock prices, handling missing values intelligently\"\"\"\n",
    "    # Read stock prices\n",
    "    stock_df = pd.read_csv(stock_prices_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    stock_df['Date'] = pd.to_datetime(stock_df['Date']).dt.date\n",
    "    articles_df['date'] = pd.to_datetime(articles_df['date']).dt.date\n",
    "    \n",
    "    # Create a mapping of company names to stock symbols\n",
    "    company_to_symbol = {\n",
    "        'tesla': 'TSLA',\n",
    "        'apple': 'AAPL',\n",
    "        'google': 'GOOGL',\n",
    "        'nvidia': 'NVDA',\n",
    "        'microsoft': 'MSFT'\n",
    "    }\n",
    "    \n",
    "    # Initialize the merged DataFrame\n",
    "    merged_data = []\n",
    "    \n",
    "    # For each company and date in articles\n",
    "    for company in articles_df['company'].unique():\n",
    "        company_articles = articles_df[articles_df['company'] == company]\n",
    "        symbol = company_to_symbol[company]\n",
    "        \n",
    "        # Get stock data for this company\n",
    "        company_stocks = stock_df[stock_df['Ticker'] == symbol].copy()\n",
    "        \n",
    "        # Create a date range for this company\n",
    "        min_date = min(company_articles['date'].min(), company_stocks['Date'].min())\n",
    "        max_date = max(company_articles['date'].max(), company_stocks['Date'].max())\n",
    "        date_range = pd.date_range(start=min_date, end=max_date, freq='D').date\n",
    "        \n",
    "        # Create a DataFrame with all dates\n",
    "        all_dates = pd.DataFrame({'date': date_range})\n",
    "        \n",
    "        # Merge articles\n",
    "        articles_merged = pd.merge(all_dates, company_articles, on='date', how='left')\n",
    "        \n",
    "        # Merge stock prices\n",
    "        stocks_merged = pd.merge(all_dates, company_stocks, \n",
    "                               left_on='date', right_on='Date', how='left')\n",
    "        \n",
    "        # Forward fill missing stock prices\n",
    "        stocks_merged = stocks_merged.sort_values('date')\n",
    "        stocks_merged = stocks_merged.fillna(method='ffill')\n",
    "        \n",
    "        # Backward fill any remaining missing values (for earliest dates)\n",
    "        stocks_merged = stocks_merged.fillna(method='bfill')\n",
    "        \n",
    "        # Combine the data\n",
    "        for _, row in articles_merged.iterrows():\n",
    "            date = row['date']\n",
    "            stock_data = stocks_merged[stocks_merged['date'] == date].iloc[0]\n",
    "            \n",
    "            merged_data.append({\n",
    "                'date': date,\n",
    "                'company': company,\n",
    "                'symbol': symbol,\n",
    "                'article_count': row['article_count'],\n",
    "                'avg_sentiment': row['avg_sentiment'],\n",
    "                'open': stock_data['Open'],\n",
    "                'high': stock_data['High'],\n",
    "                'low': stock_data['Low'],\n",
    "                'close': stock_data['Close'],\n",
    "                'volume': stock_data['Volume']\n",
    "            })\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    final_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Sort by date and company\n",
    "    final_df = final_df.sort_values(['date', 'company'])\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e44252",
   "metadata": {},
   "source": [
    "# 8. Main Execution  \n",
    "Run the full pipeline: fetch articles, aggregate, merge with prices, save CSV, and print summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize combined articles DataFrame\n",
    "    all_articles_df = pd.DataFrame()\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster in CLUSTERS:\n",
    "        print(f\"\\nProcessing cluster: {cluster['uri']}\")\n",
    "        print(f\"Date range: {cluster['start_date']} to {cluster['end_date']}\")\n",
    "        \n",
    "        # Connect to database\n",
    "        collection = connect_to_db(cluster['uri'])\n",
    "        \n",
    "        # Get articles for each company\n",
    "        company_articles = get_company_articles(collection, cluster['start_date'], cluster['end_date'])\n",
    "        \n",
    "        # Create daily aggregates DataFrame\n",
    "        articles_df = create_daily_aggregates(company_articles, cluster['start_date'], cluster['end_date'])\n",
    "        \n",
    "        # Append to combined DataFrame\n",
    "        all_articles_df = pd.concat([all_articles_df, articles_df], ignore_index=True)\n",
    "    \n",
    "    # Merge with stock prices\n",
    "    stock_prices_path = 'ohlcv_data_jan_june_2024.csv'\n",
    "    final_df = merge_with_stock_prices(all_articles_df, stock_prices_path)\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv('combined_analysis_jan_june_2024.csv', index=False)\n",
    "    print(\"\\nData saved to 'combined_analysis_jan_june_2024.csv'\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    for company in KEYWORDS:\n",
    "        company_data = final_df[final_df['company'] == company]\n",
    "        print(f\"\\n{company.capitalize()}:\")\n",
    "        print(f\"Total articles: {company_data['article_count'].sum()}\")\n",
    "        print(f\"Average sentiment: {company_data['avg_sentiment'].mean():.3f}\")\n",
    "        print(f\"Date range: {company_data['date'].min()} to {company_data['date'].max()}\")\n",
    "        print(f\"Days with articles: {company_data[company_data['article_count'] > 0]['date'].nunique()}\")\n",
    "        print(f\"Days with stock data: {company_data[company_data['close'].notna()]['date'].nunique()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
