{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632e074c",
   "metadata": {},
   "source": [
    "# S3‑to‑Mongo ETL Pipeline\n",
    "\n",
    "**What this notebook does**  \n",
    "This notebook implements our AWS S3 → MongoDB ingestion pipeline to:  \n",
    "1. Initialize the S3 client and load configuration.  \n",
    "2. List and sort all GZIP files in a given date prefix.  \n",
    "3. Read and parse JSON lines from each `.gz` file in S3.  \n",
    "4. Batch‑insert articles into MongoDB (via `insert_articles`) with duplicate handling.  \n",
    "5. Report totals of inserted vs. duplicate records per day.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77004292",
   "metadata": {},
   "source": [
    "# 1. Imports & Environment Setup  \n",
    "Import required libraries, load environment variables, and initialize modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mongo import insert_articles\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37343447",
   "metadata": {},
   "source": [
    "# 2. Initialize AWS S3 Client & Constants  \n",
    "Create the `boto3` S3 client and define batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eebd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "BATCH_SIZE = 500  # safe, small batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a54cf",
   "metadata": {},
   "source": [
    "# 3. List GZIP Files for a Given Day  \n",
    "List and sort all `.gz` keys under the `date_str` prefix in the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gz_files_for_day(bucket, date_str):\n",
    "    prefix = f\"{date_str}/\"  \n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    files = [obj[\"Key\"] for obj in response.get(\"Contents\", []) if obj[\"Key\"].endswith(\".gz\")]\n",
    "    return sorted(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028118ce",
   "metadata": {},
   "source": [
    "# 4. Read & Parse a GZIP File from S3  \n",
    "Download the GZIP object, decompress line by line, and parse each JSON record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db431f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gz_file_from_s3(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    gzipped_body = obj['Body']\n",
    "\n",
    "    records = []\n",
    "    with gzip.GzipFile(fileobj=gzipped_body, mode='rb') as gz:\n",
    "        for line in gz:\n",
    "            try:\n",
    "                line = line.decode(\"utf-8\", errors=\"replace\").strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data = json.loads(line)\n",
    "                records.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping corrupted line in {key}: {e}\")\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76c383",
   "metadata": {},
   "source": [
    "# 5. Process & Insert Articles for One Day  \n",
    "For each `.gz` file on `date_str`, read records in batches and call `insert_articles`.  \n",
    "Returns total inserted and duplicate counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a626b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles_for_day(bucket, date_str):\n",
    "    keys = list_gz_files_for_day(bucket, date_str)\n",
    "\n",
    "    total_inserted = 0\n",
    "    total_duplicates = 0\n",
    "    pending_batch = []\n",
    "\n",
    "    for key in keys:\n",
    "        print(f\"Processing file: {key}\")\n",
    "        articles = read_gz_file_from_s3(bucket, key)\n",
    "        if articles:\n",
    "            pending_batch.extend(articles)\n",
    "\n",
    "            if len(pending_batch) >= BATCH_SIZE:\n",
    "                inserted, duplicates = insert_articles(pending_batch)\n",
    "                total_inserted += inserted\n",
    "                total_duplicates += duplicates\n",
    "                pending_batch = []\n",
    "                time.sleep(1)  # let Mongo breathe\n",
    "\n",
    "    if pending_batch:\n",
    "        inserted, duplicates = insert_articles(pending_batch)\n",
    "        total_inserted += inserted\n",
    "        total_duplicates += duplicates\n",
    "\n",
    "    return total_inserted, total_duplicates\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
